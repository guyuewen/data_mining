数据挖掘

[TOC]



# 口号

--接下来我们将进行数据挖掘的项目，将为期二个月，来获取相关的知识和能力！

人呀，一定要不蒸馒头，争口气！



# -------------------------------------1



# 数据探索阶段

## Python 主要数据探索函数

### Pandas统计函数

**Pandas主要统计特征函数**

|   方法名   |                     函数功能                     |
| :--------: | :----------------------------------------------: |
|   sum()    |          计算数据样本的总和（按列计算）          |
|   mean()   |             计算数据样本的算数平均数             |
|   var()    |                计算数据样本的方差                |
|   std()    |               计算数据样本的标准差               |
|   corr()   |   计算数据样本的Spearman(Pearson)相关系数矩阵    |
|   cov()    |             计算数据样本的协方差矩阵             |
|   skew()   |              样本值的偏度（三阶矩）              |
|   kurt()   |              样本值的峰度（四阶矩）              |
| describe() | 给出样本的基本描述（基本统计量如均值、标注差等） |


**Pandas累积统计特征函数**

|    方法名    |        函数功能         |
| :----------: | :---------------------: |
| cumsum(`n`)  |   依次给出前n个数的和   |
| cumprod(`n`) |   依次给出前n个数的积   |
| cummax(`n`)  | 依次给出前n个数的最大值 |
| cummin(`n`)  | 依次给出前n个数的最小值 |


**Pandas滚动统计特征函数**

|     方法名     |                  函数功能                   |
| :------------: | :-----------------------------------------: |
| rolling_sum()  |       计算数据样本的总和（按列计算）        |
| rolling_mean() |            数据样本的算数平均数             |
| rolling_var()  |             计算数据样本的方差              |
| rolling_std()  |            计算数据样本的标准差             |
| rolling_corr() | 计算数据样本的Spearman(Pearson)相关系数矩阵 |
| rolling_cov()  |          计算数据样本的协方差矩阵           |
| rolling_skew() |           样本值的偏度（三阶矩）            |
| rolling_kurt() |            样本的峰度（四阶矩）             |

调用方法：pd.rolling_mean(D, k)，意思是每k列计算依次均值，滚动计算。


### Pandas绘图函数

Pandas 基于 Matplotlib并对某些命令进行了简化，因此作图通常是 Matplotlib 和 Pandas 相互结合着使用。

**Pandas主要统计作图函数**

|       函数名       |                 函数功能                 |
| :----------------: | :--------------------------------------: |
|       plot()       |          绘制线性二维图，折线图          |
|       pie()        |                绘制饼形图                |
|       hist()       | 绘制二维条形直方图，可显示数据的分配情况 |
|     boxplot()      |           绘制样本数据的箱型图           |
| plot(logy = True)  |            绘制y轴的对数图形             |
| plot(yerr = error) |              绘制误差条形图              |


```python
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
```













# 数据探索

通过检验数据集的数据质量、绘制图表、计算某些特征量等手段，对样本数据集的结构和规律进行分析的过程就是数据探索。数据探索有助于选择合适的数据预处理和建模方法

## 数据质量分析

数据质量分析的主要任务是检查原始数据中是否存在涨数据，涨数据一般是指不符合要求，以及不能直接进行相应分析的数据。在常见的数据挖掘工作中，涨数据包括如下内容：

- 缺失值
- 异常值
- 不一致的值
- 重复数据及含有特殊符号（如 #、￥、*）的数据

### 缺失值分析

数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。

**（1）缺失值产生的原因**

- 有些信息暂时无法获取，或者获取信息的代价太大。
- 有些信息是被遗漏的，这可能是人为的或者某些意外造成的。例如忘记填写或设备故障等。
- 属性值不存在。在某些情况下，对一些对象来说某些属性值是不存在的，如一个未婚者的配偶姓名、一个儿童的固定收入等。

**（2）缺失值的影响**

- 数据挖掘建模将丢失大量的有用信息。
- 数据挖掘模型所表现数的不确定性更加显著，模型中蕴含的规律更难把握。
- 包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。

**（3）缺失值分析**

- 使用简单的统计分析，可以得到含有缺失值的属性个数，以及每个属性的未缺失数，缺失数于缺失率等。
- 对于缺失值的处理后面会单独讲。

### 异常值分析

异常值分析是检验数据是否有录入错误以及含有不合常理的数据。异常值是指样本中的个别 值，其数值明显偏离其余的观测值。异常值也称为离群点。

**（1）简单统计量分析**

- 可以先对变量做一个描述性统计，例如用最大值和最小值来判断这个变量的取值是否超出了合理的范围。

**（2)3σ原则**

- 如果数据服从正态分布，在3$\sigma$原则下，异常值被定义为一组观测值中于平均值的偏差超过3倍标准差的值。在标准正态分布的假设下，距离平均值3$\sigma$之外的值出现的概率为
  $$
  P(|x-\mu|>3\sigma) \le 0.003
  $$
  ，属于极个别的小概率事件。

- 如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。

**（3）箱型图分析**

- 箱型图提供了识别异常值的一个标准：异常值通常被定义为小于$Q_L - 1.5IQR$或大于$Q_U +1.5IQR$的值。$Q_L$称为下四分位数，表示全部观察值中有四分之一的数据取值比它小；$Q_U$称为上四分为数，表示全部观察值中有四分之一的数据取值比它大；$IQR$称为四分位数间距，是上四分位数$Q_U$与下四分位数$Q_L$之差，期间包含了全部观察值的一半。&nbsp;箱线图真实客观地表现数据分布的本来面貌；它判断异常值的标准以四分位数和四分位距为基础，四分位数具有一定的鲁棒性，在识别异常值方面有一定的优越性。


```python
data = pd.read_excel('data/catering_sale.xls', index_col=u'日期') # 读取餐饮数据，指定“日期”列为索引列。
data.describe() # 查看数据的基本情况
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>销量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2755.214700</td>
    </tr>
    <tr>
      <th>std</th>
      <td>751.029772</td>
    </tr>
    <tr>
      <th>min</th>
      <td>22.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2451.975000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2655.850000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3026.125000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>9106.440000</td>
    </tr>
  </tbody>
</table>
</div>

```python
def programmer_1(data):
    plt.figure()
    # 画箱线图
    p = data.boxplot(return_type='dict')
    x = p['fliers'][0].get_xdata()
    y = p['fliers'][0].get_ydata()
    y.sort()
    print(y)

    for i in range(len(x)):
        # 处理临界情况， i=0时
        temp = y[i] - y[i - 1] if i != 0 else -78 / 3
        # 添加注释, xy指定标注数据，xytext指定标注的位置（所以需要特殊处理）
        plt.annotate(y[i], xy=(x[i], y[i]), xytext=(x[i] + 0.05 - 0.8 / temp, y[i]))
    plt.show()

programmer_1(data)
```

    [   22.      51.      60.     865.    4060.3   4065.2   6607.4   9106.44]



![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_9_1.png)


根据上面的箱型图，结合具体业务可以把865、4060.3、4065.2归为正常值，将22、51、60、6607.4、9406.44归为异常值。最后确定过滤规则为：日销量在400以下5000以上则属于异常值。

### 一致性分析

数据不一致性是指数据的矛盾性、不相容性。不一致数据的产生主要发生在数据集成的过程中，这可能是由于被挖掘数据是来自于从不同的数据源、对于重复存放的数据未能进行一致性更新造成的。例如。两张表中存储了用户的电话号码，但在用户的电话号码发生改变时只更新了一张表中的数据，那么这两张表中就有了不一致的数据。

##  数据特征分析

对数据进行质量分析以后，接下来可通过绘制图标、计算某些特征量等手段进行数据的特征分析。

### 分布分析

分布分析能揭示数据的分布特征和分布类型。

- 对于定量数据，可以绘制频率分布表、绘制频率分布直方图、绘制茎叶图进行直观分析。
- 对于定性分类数据，可用饼图和条形图直观地显示分布情况。

#### 1.定量数据的分布分析

对于定量变量而言，选择“组数”和“组宽”是做频率分布分析时最主要的问题，一般按照以下步骤进行。

- 1.求极差
- 2.决定组距于组数
- 3.决定分点
- 4.列出频率分布表
- 5.绘制频率分布直方图

遵循的主要原则如下

- 1.各组之间必须是互相排斥的
- 2.各组必须将所有的数据包含在内
- 3.各组的组宽最好相等

以“捞器生鱼片”菜品举例：


```python
data.head()
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>销量</th>
    </tr>
    <tr>
      <th>日期</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2015-03-01</th>
      <td>51.0</td>
    </tr>
    <tr>
      <th>2015-02-28</th>
      <td>2618.2</td>
    </tr>
    <tr>
      <th>2015-02-27</th>
      <td>2608.4</td>
    </tr>
    <tr>
      <th>2015-02-26</th>
      <td>2651.9</td>
    </tr>
    <tr>
      <th>2015-02-25</th>
      <td>3442.1</td>
    </tr>
  </tbody>
</table>
</div>




**1.求极差**

极差 = 最大值 - 最小值


```python
xse = data['销量']
range_mm = np.max(xse) - np.min(xse)
range_mm
```




    9084.4400000000005



**2.分组**

这里根据业务数据的含义，可取组距为500.<br>
组数 = 极差 / 组距


```python
num_split = range_mm / 500
num_split
```




    18.168880000000001



**3.决定分点**

|          | 分布区间    |      |
| -------- | ----------- | ---- |
| [0, 500) | [500, 1000) | ...  |

**4.列出频率分布表**

**5.绘制频率分布直方图**


```python
range_list = list(range(0, 5001, 500))
data_cut = pd.cut(xse.values, range_list, right=False)  # 分组区间
frequency = data_cut.value_counts()  # 区间-个数

frequency.plot(kind='bar')
range_list = pd.cut(xse, range_list, right=False)
data['区间'] = range_list.values
data.groupby('区间').median()
data.groupby('区间').mean()  # 每个区间平均数

frequency_df = pd.DataFrame(frequency, columns=['频数'])
frequency_df['频率f'] = frequency_df / frequency_df['频数'].sum()
frequency_df['频率%'] = frequency_df['频率f'].map(lambda x: '%.2f%%' % (x * 100))
frequency_df['累计频率f']=frequency_df['频率f'].cumsum()
frequency_df['累计频率%']=frequency_df['累计频率f'].map(lambda x:'%.4f%%'%(x*100))
print(frequency_df)
```

                  频数       频率f     频率%     累计频率f      累计频率%
    [0, 500)       3  0.015152   1.52%  0.015152    1.5152%
    [500, 1000)    1  0.005051   0.51%  0.020202    2.0202%
    [1000, 1500)   0  0.000000   0.00%  0.020202    2.0202%
    [1500, 2000)   1  0.005051   0.51%  0.025253    2.5253%
    [2000, 2500)  53  0.267677  26.77%  0.292929   29.2929%
    [2500, 3000)  87  0.439394  43.94%  0.732323   73.2323%
    [3000, 3500)  44  0.222222  22.22%  0.954545   95.4545%
    [3500, 4000)   7  0.035354   3.54%  0.989899   98.9899%
    [4000, 4500)   2  0.010101   1.01%  1.000000  100.0000%
    [4500, 5000)   0  0.000000   0.00%  1.000000  100.0000%



![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_23_1.png)


#### 2.定性数据的分布分析

对于定性变量，常常根据变量的分类类型来分组，可以采用饼图和条形图来描述定性变量的分布。<br>
饼图每一部分的大小于每一类型的频数成正比；条形图的高度代表每一类型分百分比或频数，它的宽度没有意义。


```python
labels = 'Frogs','Hogs','Dogs','Logs'
sizes = [15,30,45,10]
explode = (0,0.1,0,0) # 0.1表示将Hogs那一块凸显出来
plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=False, startangle=90) #startangle表示饼图的起始角度

plt.axis('equal')  # 这行让长宽比例相等
plt.show()
```


![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_25_0.png)





```python
fig = plt.subplot(111)
width = 0.5
x_bar=np.arange(4)
fig.bar(x=x_bar, height=sizes, width=width, color='lightblue')
fig.set_xticks(x_bar)
fig.set_xticklabels(labels)
fig.set_ylabel("sales")
fig.set_ylim(0, 50)
fig.set_title("The Sales in 2018")
fig.grid(True)
plt.show()
```


![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_27_0.png)


### 对比分析

对比分析是值把两个相互联系的指标进行比较，从数量上展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。特别适用于指标间的横纵向比较、时间序列的比较分析。

对比分析主要有一下两种形式：<br>

**（1）绝对数比较**<br>
绝对数比较是利用绝对数进行对比，从而寻找差异的一种方法。

**（2）相对数比较**<br>
相对数比较是由两个有联系的指标对比计算的，用以反映客观现象之间数量联系程度的指标，其数值表现形式为相对数。<br>

- **结构相对数**：将同一总体内的部分数值与全部数值对比求得比重，用以说明失误的性质、结构或质量。如居民食品支出总额比重、产品合格率等。
- **比例相对数**：将同一总体内不同部分的数值进行对比，表面总体内各部分的比例关系。如人口性别比例、投资与消费比例等。
- **比较相对数**：将同一时期两个性质相同的指标数值进行对比，说明同类现象在不同空间条件下的数量对比关系。如不同地区商品价格对比，不同行业、不同企业间某项指标对比等。
- **强度相对数**：将两个性质不同但有一定联系的总量指标进行对比，用以说明现象的强度、密度和普遍程度。如人均生产总值用“元/人”表示，人口密度用“人/平方公里”表示，也有用百分数或千分数标示的，如人口出生率%。表示。
- **计划完成度相对数**：是某一时期实际完成数与计划数的对比，用以说明计划完成程度。
- **动态相对数**：将同以现象在不同时期的指标数值进行对比，用以说明发展方向和变化速度。如发展速度、增长速度等。




```python
x1 = [32000, 39000, 42000, 30000, 20000, 25000, 31000, 26000, 28000, 30000, 33000, 39000]
x2 = [38000, 42000, 43000, 31000, 25000, 21000, 29000, 30000, 31000, 29000, 26000, 36000]
plt.figure(figsize=(15, 10))
fig = plt.subplot()
fig.plot(np.arange(12), x1, label='2017')
fig.plot(np.arange(12), x2, label='2018')
fig.set_xticks(np.arange(12))
fig.set_xticklabels(['1 month', '2 month', '3 month', '4 month', '5 month', '6 month', '7 month', '8 month', '9 month', '10 month', '11 month', '12 month'])
fig.set_ylim(15000, 50000)
fig.set_ylabel(u"sales (yuan)")
fig.set_title(u"The Sales in 2017 and 2018")
fig.legend(['2017', '2018'], loc=2, ncol=1)
plt.show()
```


![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_29_0.png)


### 统计量分析

用统计指标对定量数据进行统计描述，常从集中趋势和离中趋势两个方面进行分析。

#### 集中趋势度量

- **1.均值**<br>
  均值是所有数据的平均值。<br>
  如果求n个原始观察数据的平均值，计算公式为：
  $$
  \rm mean(x) = \bar x = \frac{\sum x_i}{n}
  $$
  有时会用到加权平均值
  $$\rm mean(x) = \bar x = \frac{\sum \omega_{i}x_i}{\sum \omega_i}$$
  类似地，频率分布表的平均数计算公式：
  $$\rm mean(x) =\bar x = \sum \it f_{i}x_i$$
  式中，$x_i$为第i个组段的组中值；$\it f_i$为第i组的频率。

  有时会用到加权平均值
  $$
  \rm mean(x) = \bar x = \frac{\sum \omega_{i}x_i}{\sum \omega_i}
  $$
  类似地，频率分布表的平均数计算公式：
  $$
  \rm mean(x) =\bar x = \sum \it f_{i}x_i
  $$
  式中，$x_i$为第i个组段的组中值；$\it f_i$为第i组的频率。

  式中，$x_i$为第i个组段的组中值；$\it f_i$为第i组的频率。

  类似地，频率分布表的平均数计算公式：
  $$
  \rm mean(x) =\bar x = \sum \it f_{i}x_i
  $$
  式中，$x_i$为第i个组段的组中值；$\it f_i$为第i组的频率。

  式中，$x_i$为第i个组段的组中值；$\it f_i$为第i组的频率。

均值对极端值很敏感，如果数据中存在极端值或者数据是偏态分布的，那么均值就不能很好地度量数据的集中程度。为了消除少数极端值的影响，可以使用`截断均值`或者`中位数`来度量数据的集中趋势。截断均值是去掉高、低计算值之后的平均数。


- **2.中位数**<br>
  中位数是将一组数据观察值按从小到大的顺序排列，位于中间的那个数。<br>

将某一数据集$x:(x_1,x_2, \dots, x_n)$按从小到大排列：$x_{(1)}, x_{(2)}, \dots, x_{(n)}$。<br>

当n为奇数时
$$M = x_{\frac {n+1}{2}}$$
当n为偶数时
$$M = \frac{1}{2}(x_{\frac{n}{2}} + x_{\frac{n+1}{2}})$$

- **3.众数**<br>
  众数是指数据集中出现最频繁的值。众数并不经常用来度量定性变量的中心位置，更适用于定性变量。众数不具有偶唯一性，一般用于离散型变量而非连续型变量。

#### 离中趋势度量

- **1.极差**<br>
  极差对数据集的极端值非常敏感，并且忽略了位于最大值与最小值之间的数据的分布情况。

- **2.标准差**<br>
  标准差度量数据偏离均值的程度，计算公式为：
  $$s = \sqrt{\frac{\sum (x_i - \bar x)^2}{n}}$$

- **3.变异系数**<br>
  变异系数度量标准差相对于均值的离中趋势，计算公式为：
  $$\rm CV = \frac{s}{\bar x} \times 100\%$$    

> 变异系数主要用来比较两个或多个具有不同单位或不同波动幅度的数据集的离中趋势。

- **4.四分位数间距**<br>
  四分位数间距，是上四分位数$Q_U$与下四分位数$Q_L$之差，其间包含了全部观察值的一半。其值越大，说明数据的变异程度越大；反之，说明变异程度越小。


```python
data.describe()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>销量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2755.214700</td>
    </tr>
    <tr>
      <th>std</th>
      <td>751.029772</td>
    </tr>
    <tr>
      <th>min</th>
      <td>22.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2451.975000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2655.850000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3026.125000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>9106.440000</td>
    </tr>
  </tbody>
</table>
</div>




### 周期性分析

周期性分析是探索某个变量是否随着时间变化而呈现出某种变化趋势。时间尺度相对较长的周期性趋势有年度周期性趋势、季节性周期趋势，相对较短的有月度周期性趋势、周度周期性趋势，甚至更短的天、小时周期性趋势。

### 贡献度分析（帕累托分析）

贡献度分析又称为帕累托分析，它的原理是帕累托法则，又称为20/80定律。同样的投入放在不同的地方会产生不同的效益。例如，对一个公司来讲，80%的利润常常来自20%最畅销的产品，而对其他80%的产品只产生了20%利润。

下面展示了某餐厅，海鲜系列10个菜品A1～A10某个月的盈利额（已按照从大到小顺序排列）


```python
import matplotlib
from matplotlib.font_manager import *
myfont = FontProperties(fname='/home/heolis/SIMSUN.TTC')
matplotlib.rcParams['axes.unicode_minus']=False



dish_profit = 'data/catering_dish_profit.xls' #餐饮菜品盈利数据
data = pd.read_excel(dish_profit, index_col = u'菜品名')
data = data[u'盈利'].copy()
data.sort_values(ascending = False)

plt.figure()
data.plot(kind='bar')
plt.ylabel(u'盈利（元）',fontproperties=myfont)
p = 1.0*data.cumsum()/data.sum()
p.plot(color = 'r', secondary_y = True, style = '-o',linewidth = 2)
plt.annotate(format(p[6], '.4%'), xy = (6, p[6]), xytext=(6*0.9, p[6]*0.9), arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2")) #添加注释，即85%处的标记。这里包括了指定箭头样式。
plt.ylabel(u'盈利（比例）',fontproperties=myfont)
plt.title(u'帕累托图', fontproperties=myfont)
plt.show()
```


![png](C:/Users/Lenovo/DataMiningNotesAndPractice-master/实战篇/1.数据探索/output_41_0.png)


由上图可知，菜品A1~A7共7个菜品，占菜品种类数的70%，总盈利额占该月盈利额的85.0033%.根据帕累托法则，应该增加菜品A1~A7的成本投入，减少对菜品A8~A10的投入以获得更高的盈利额。

### 相关性分析

分析连续变量之间线性相关程度的强弱。

#### 1.绘制散点图

判断两个变量是否具有线性相关关系的最直观的方法是直接绘制散点图。
![相关关系图示](http://www.2cto.com/uploadfile/2015/1214/20151214020756652.png)

#### 2.绘制散点图矩阵

需要同时考虑多个变量间的相关关系时，利用散点图矩阵同时绘制各变量间的散点图。
![散点图矩阵](http://www.2cto.com/uploadfile/2015/1214/20151214022406586.png)

#### 计算相关系数

在二元变量的相关分析过程中比较常用的有Pearson 相关系数、Spearman 质相关系数、判定系数。

- **1.Pearson 相关系数**
  
  一般用于分析两个连续性变量之间的关系，其计算公式如下。
  $$
  r = \frac{\sum ^n_{i = 1}(x_i - \bar x)(y_i - \bar y)}{\sqrt{ \sum ^n_{i = 1}(x_i - \bar x)^2 \sum ^n_{i = 1}(y_i - \bar y)^2}}
  $$
  

相关系数r的取值范围：
$$
-1 \le r \le 1
$$

$$
\begin{cases}r > 0 &为正相关 \\ r < 0 &为负相关 \\ |r| = 0 &表示不存在线性关系 \\ |r| = 1 &表示完全线性关系 \end{cases}
$$
0<|r|<1 表示存在不同程度线性相关：
$$
\begin{cases}|r| \le 0.3 & 为不存在线性相关 \\ 0.3 < |r| \le 0.5 & 为低度线性相关 \\ 0.5 < |r| \le 0.8 &为显著线性相关 \\ |r| > 0.8 &为高度线性相关 \end{cases}
$$
Pearson线性相关系数要求连续变量服从正态分布。Pearson相关只有在变量具有线性关系时才完全相关。

Pearson线性相关系数要求连续变量服从正态分布。Pearson相关只有在变量具有线性关系时才完全相关。

0<|r|<1 表示存在不同程度线性相关：
$$
\begin{cases}|r| \le 0.3 & 为不存在线性相关 \\ 0.3 < |r| \le 0.5 & 为低度线性相关 \\ 0.5 < |r| \le 0.8 &为显著线性相关 \\ |r| > 0.8 &为高度线性相关 \end{cases}
$$
Pearson线性相关系数要求连续变量服从正态分布。Pearson相关只有在变量具有线性关系时才完全相关。

Pearson线性相关系数要求连续变量服从正态分布。Pearson相关只有在变量具有线性关系时才完全相关。

- **2.Spearman秩相关系数**
  
  不服从正态分布的变量、分类或等级变量之间的关联性可采用Spearman秩相关系数，也称等级相关系数来描述。
  其计算公式如下：
  $$
  r_s = 1 - \frac{6\sum^n_{i=1}(R_i - Q_i)^2}{n(n^2 - 1)}
  $$
  


> 对两个变量成对的取值分别按照从小到大（或者从大到大小）顺序编秩，$R_i$代表$x_i$的秩次，$Q_i$代表$y_i$的秩次，$R_i-Q_i$为$x_i$、$y_i$的秩次之差。<br>
> 由于一个变量的相同的取值必须有相同的秩次，所以在计算中采用的秩次是排序后所在位置的平均值。<br>
> 只要两个变量具有严格单调的函数关系，那么它们就是完全Spearman相关的。研究表明，在正态分布假定下，Spearman秩相关系数与Pearson相关系数在效率上是等价的，而对于连续测量数据，更适合用Pearson相关系数来进行分析。

- **3.判定系数**<br>
  判定系数是相关系数的平方，用$r^2$表示；用来衡量回归方程对y的解释程度。判定系数取值范围：
  $$
  $0≤r2≤1$。$r^2$越接近于1，表明x与y之间的相关性越强；$r^2$越接近于0，表明两个变量之间几乎没有直线相关关系。
  $$
  


```python
catering_sale = 'data/catering_sale_all.xls' #餐饮数据，含有其他属性
data = pd.read_excel(catering_sale, index_col = u'日期') #读取数据，指定“日期”列为索引列

data.corr() #相关系数矩阵，即给出了任意两款菜式之间的相关系数
data.corr()[u'百合酱蒸凤爪'] #只显示“百合酱蒸凤爪”与其他菜式的相关系数
```


    百合酱蒸凤爪     1.000000
    翡翠蒸香茜饺     0.009206
    金银蒜汁蒸排骨    0.016799
    乐膳真味鸡      0.455638
    蜜汁焗餐包      0.098085
    生炒菜心       0.308496
    铁板酸菜豆腐     0.204898
    香煎韭菜饺      0.127448
    香煎罗卜糕     -0.090276
    原汁原味菜心     0.428316
    Name: 百合酱蒸凤爪, dtype: float64


```python
data[u'百合酱蒸凤爪'].corr(data[u'翡翠蒸香茜饺']) #计算“百合酱蒸凤爪”与“翡翠蒸香茜饺”的相关系数
```


    0.0092058030518365284





# 数据预处理

**Python 主要数据预处理函数**

|   函数名    |                         函数功能                         |    所属库    |
| :---------: | :------------------------------------------------------: | :----------: |
| interpolate |                    一维、高维数据插值                    |    Scipy     |
|   unique    | 去除数据中的重复元素，得到单值元素列表，它是对象的方法名 | Pandas/Numpy |
|   isnull    |                       判断是否空值                       |    Pandas    |
|   notnull   |                      判断是否非空值                      |    Pandas    |
|     PCA     |               对指标变量矩阵进行主成分分析               | Scikit-Learn |
|   random    |                       生成随机矩阵                       |    Numpy     |

## 基本流程

- **1.检查数据**

- **2.缺失值处理**

- **3.异常值处理**

- **4.数据集成**

- **5.数据变换**

- **6.数据规约**

- **7.变量表示**

  

## 检查数据

读入数据中的第一步，就是检查数据，看看都有哪些变量，这些变量分布如何，是不是存在错误的观测。

### 服装消费者数据描述：

- age:年龄
- gender:性别
- income:收入
- house:是否有房子
- store_exp:实体店消费额
- online_exp:在线消费额
- store_trans:在实体店交易次数
- online_trans:在线交易次数
- Q1~Q10:问卷的10个问题（非常不同意：1；有点不同意：2；中立/不知道：3；有点同意：4；非常同意：5）
- segment:消费者分组(价格敏感：Price；炫耀性消费：Conspicuous；质量：Quality；风格：Style)


```python
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

%matplotlib inline
```


```python
data = pd.read_csv("data/segdata.csv")
```


```python
data.head()
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>gender</th>
      <th>income</th>
      <th>house</th>
      <th>store_exp</th>
      <th>online_exp</th>
      <th>store_trans</th>
      <th>online_trans</th>
      <th>Q1</th>
      <th>Q2</th>
      <th>Q3</th>
      <th>Q4</th>
      <th>Q5</th>
      <th>Q6</th>
      <th>Q7</th>
      <th>Q8</th>
      <th>Q9</th>
      <th>Q10</th>
      <th>segment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>57</td>
      <td>Female</td>
      <td>120963.400958</td>
      <td>Yes</td>
      <td>529.134363</td>
      <td>303.512475</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>Price</td>
    </tr>
    <tr>
      <th>1</th>
      <td>63</td>
      <td>Female</td>
      <td>122008.104950</td>
      <td>Yes</td>
      <td>478.005781</td>
      <td>109.529710</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>Price</td>
    </tr>
    <tr>
      <th>2</th>
      <td>59</td>
      <td>Male</td>
      <td>114202.295294</td>
      <td>Yes</td>
      <td>490.810731</td>
      <td>279.249582</td>
      <td>7</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>Price</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60</td>
      <td>Male</td>
      <td>113616.337078</td>
      <td>Yes</td>
      <td>347.809004</td>
      <td>141.669752</td>
      <td>10</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>Price</td>
    </tr>
    <tr>
      <th>4</th>
      <td>51</td>
      <td>Male</td>
      <td>124252.552787</td>
      <td>Yes</td>
      <td>379.625940</td>
      <td>112.237177</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>4</td>
      <td>Price</td>
    </tr>
  </tbody>
</table>
</div>


```python
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1000 entries, 0 to 999
    Data columns (total 19 columns):
    age             1000 non-null int64
    gender          1000 non-null object
    income          816 non-null float64
    house           1000 non-null object
    store_exp       1000 non-null float64
    online_exp      1000 non-null float64
    store_trans     1000 non-null int64
    online_trans    1000 non-null int64
    Q1              1000 non-null int64
    Q2              1000 non-null int64
    Q3              1000 non-null int64
    Q4              1000 non-null int64
    Q5              1000 non-null int64
    Q6              1000 non-null int64
    Q7              1000 non-null int64
    Q8              1000 non-null int64
    Q9              1000 non-null int64
    Q10             1000 non-null int64
    segment         1000 non-null object
    dtypes: float64(3), int64(13), object(3)
    memory usage: 148.5+ KB



```python
data.describe()
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>store_exp</th>
      <th>online_exp</th>
      <th>store_trans</th>
      <th>online_trans</th>
      <th>Q1</th>
      <th>Q2</th>
      <th>Q3</th>
      <th>Q4</th>
      <th>Q5</th>
      <th>Q6</th>
      <th>Q7</th>
      <th>Q8</th>
      <th>Q9</th>
      <th>Q10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000.000000</td>
      <td>816.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38.840000</td>
      <td>113543.065222</td>
      <td>1356.850523</td>
      <td>2120.181187</td>
      <td>5.350000</td>
      <td>13.546000</td>
      <td>3.101000</td>
      <td>1.823000</td>
      <td>1.992000</td>
      <td>2.763000</td>
      <td>2.945000</td>
      <td>2.448000</td>
      <td>3.434000</td>
      <td>2.396000</td>
      <td>3.085000</td>
      <td>2.320000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>16.416818</td>
      <td>49842.287197</td>
      <td>2774.399785</td>
      <td>1731.224308</td>
      <td>3.695559</td>
      <td>7.956959</td>
      <td>1.450139</td>
      <td>1.168348</td>
      <td>1.402106</td>
      <td>1.155061</td>
      <td>1.284377</td>
      <td>1.438529</td>
      <td>1.455941</td>
      <td>1.154347</td>
      <td>1.118493</td>
      <td>1.136174</td>
    </tr>
    <tr>
      <th>min</th>
      <td>16.000000</td>
      <td>41775.637023</td>
      <td>-500.000000</td>
      <td>68.817228</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>25.000000</td>
      <td>85832.393634</td>
      <td>204.976456</td>
      <td>420.341127</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.750000</td>
      <td>1.000000</td>
      <td>2.500000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>36.000000</td>
      <td>93868.682835</td>
      <td>328.980863</td>
      <td>1941.855436</td>
      <td>4.000000</td>
      <td>14.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>53.000000</td>
      <td>124572.400926</td>
      <td>597.293077</td>
      <td>2440.774823</td>
      <td>7.000000</td>
      <td>20.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>300.000000</td>
      <td>319704.337941</td>
      <td>50000.000000</td>
      <td>9479.442310</td>
      <td>20.000000</td>
      <td>36.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
</div>




由上面的数据发现有什么问题吗？

- `age`：的最大值是300，这不大可能。
- `income`：存在缺失值（816/1000）
- `store_exp`：不应该存在负数、还可能存在离群值，最大消费为50000
- online_exp：看上去没什么问题
- store_trans和online_trans：看上去还比较合理
- Q1～Q10：值的范围都在1~5之内，貌似没问题

那怎么处理这些错误的值呢？这取决于你的实际情况，如果你的样本量很大，不在乎这几个样本，那么就可以删除这些不合理的值。在这里，由于我们只有1000个样本，并且获取这些数据不易，所以得想办法填补这些异常值。我们先把这些值设为缺失状态。


```python
# 将错误的年龄观测设置为缺失值
data['age'].loc[data['age'] > 100] = np.nan
# 将错误的实体店购买设置为缺失值
data['store_exp'].loc[data['store_exp'] < 0] = np.nan
```

    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
      self._setitem_with_indexer(indexer, value)



```python
# 查看处理后数据的情况
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1000 entries, 0 to 999
    Data columns (total 19 columns):
    age             999 non-null float64
    gender          1000 non-null object
    income          816 non-null float64
    house           1000 non-null object
    store_exp       999 non-null float64
    online_exp      1000 non-null float64
    store_trans     1000 non-null int64
    online_trans    1000 non-null int64
    Q1              1000 non-null int64
    Q2              1000 non-null int64
    Q3              1000 non-null int64
    Q4              1000 non-null int64
    Q5              1000 non-null int64
    Q6              1000 non-null int64
    Q7              1000 non-null int64
    Q8              1000 non-null int64
    Q9              1000 non-null int64
    Q10             1000 non-null int64
    segment         1000 non-null object
    dtypes: float64(4), int64(12), object(3)
    memory usage: 148.5+ KB

## 缺失值处理

处理缺失值方法可分为3类：删除记录、数据插补和不处理。

缺失值处理要视情况而定，没有某个方法永远比其他方法好。

在决定处理缺失值值的方法之前，要先了解缺失的原因等关于缺失的辅助信息。

- **缺失是随机发生的吗？如果是，可以用中位数/众数进行填充，也可以使用均值填充。**
- **或者说缺失其实是有潜在发生机制的吗？比如年龄大的人在问卷调查中更不愿意透露年龄，这样关于年龄的缺失就不是随机发生的，如果使用均值或者中位数进行填补可能会产生很大偏差。**这时需要利用年龄和其他自变量的关系对缺失值进行估计。比如可以基于那些没有缺失值的数据来建模，然后拟合模型预测缺失值。

如果建模的目的是预测，大部分情况下不会很严格地研究缺失机制（缺失机制很明显的时候除外），在缺失机制不太清楚的情况下，可以当成随机缺失进行填补（使用均值中位数或者用K-近邻）

**常用的插补方法**

|       插补方法       |                           方法描述                           |
| :------------------: | :----------------------------------------------------------: |
| 均值/中位数/众数插补 |   根据属性的类型，用该属性取值的平均数/中位数/众数进行插补   |
|      使用固定值      | 将缺失的属性值用一个常量替换。如广州一个工厂普通外来务工人员的“基本工资”属性的缺失值，可以用当年广州市普通外来务工人员工资标准1895元/月填补。 |
|      最近临填补      |        在记录中找到与缺失样本最近的样本的该属性值插补        |
|       回归方法       | 对带有缺失值的变量，根据已有数据和其有关的其他变量（因变量）的数据建立拟合模型来预测缺失的属性值 |
|        插值法        | 插值法是利用已知点建立合适的插值函数f(x)f(x)，未知值由对应点𝑥𝑖xi求出的函数值 𝑓(𝑥𝑖)f(xi)近似代替 |

### 中位数或众数填补

对于数值变量我们用中位数进行填补，对于分类变量我们用众数填补


```python
data0 = data.copy()  # 拷贝一份数据，方便对比
data0['age'].fillna(data0['age'].median(), inplace=True)
data0['income'].fillna(data0['income'].median(), inplace=True)
data0['store_exp'].fillna(data0['store_exp'].median(), inplace=True)
```


```python
data0.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1000 entries, 0 to 999
    Data columns (total 19 columns):
    age             1000 non-null float64
    gender          1000 non-null object
    income          1000 non-null float64
    house           1000 non-null object
    store_exp       1000 non-null float64
    online_exp      1000 non-null float64
    store_trans     1000 non-null int64
    online_trans    1000 non-null int64
    Q1              1000 non-null int64
    Q2              1000 non-null int64
    Q3              1000 non-null int64
    Q4              1000 non-null int64
    Q5              1000 non-null int64
    Q6              1000 non-null int64
    Q7              1000 non-null int64
    Q8              1000 non-null int64
    Q9              1000 non-null int64
    Q10             1000 non-null int64
    segment         1000 non-null object
    dtypes: float64(4), int64(12), object(3)
    memory usage: 148.5+ KB

```python
data0.describe()
```


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>income</th>
      <th>store_exp</th>
      <th>online_exp</th>
      <th>store_trans</th>
      <th>online_trans</th>
      <th>Q1</th>
      <th>Q2</th>
      <th>Q3</th>
      <th>Q4</th>
      <th>Q5</th>
      <th>Q6</th>
      <th>Q7</th>
      <th>Q8</th>
      <th>Q9</th>
      <th>Q10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
      <td>1000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38.576000</td>
      <td>109922.978863</td>
      <td>1357.680319</td>
      <td>2120.181187</td>
      <td>5.350000</td>
      <td>13.546000</td>
      <td>3.101000</td>
      <td>1.823000</td>
      <td>1.992000</td>
      <td>2.763000</td>
      <td>2.945000</td>
      <td>2.448000</td>
      <td>3.434000</td>
      <td>2.396000</td>
      <td>3.085000</td>
      <td>2.320000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>14.183702</td>
      <td>45660.371065</td>
      <td>2773.967922</td>
      <td>1731.224308</td>
      <td>3.695559</td>
      <td>7.956959</td>
      <td>1.450139</td>
      <td>1.168348</td>
      <td>1.402106</td>
      <td>1.155061</td>
      <td>1.284377</td>
      <td>1.438529</td>
      <td>1.455941</td>
      <td>1.154347</td>
      <td>1.118493</td>
      <td>1.136174</td>
    </tr>
    <tr>
      <th>min</th>
      <td>16.000000</td>
      <td>41775.637023</td>
      <td>155.810945</td>
      <td>68.817228</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>25.000000</td>
      <td>87896.274702</td>
      <td>205.060125</td>
      <td>420.341127</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.750000</td>
      <td>1.000000</td>
      <td>2.500000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>36.000000</td>
      <td>93868.682835</td>
      <td>329.795511</td>
      <td>1941.855436</td>
      <td>4.000000</td>
      <td>14.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>53.000000</td>
      <td>119455.865972</td>
      <td>597.293077</td>
      <td>2440.774823</td>
      <td>7.000000</td>
      <td>20.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>69.000000</td>
      <td>319704.337941</td>
      <td>50000.000000</td>
      <td>9479.442310</td>
      <td>20.000000</td>
      <td>36.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
    </tr>
  </tbody>
</table>
</div>


### K-近邻填补

使用KNN填补缺失值的基本思路是对于含有缺失值的样本，寻找离该样本最近的K个邻居，然后用这些邻居的观测值进行填补。由于这里是根据计算样本点之间的距离来确定邻居的，因此各个变量的标度需要统一，不然尺度大的度量在决定距离上会占主导地位。

这里仅以`income`属性为例。


```python
data1 = data.copy()
# 移除非数值型变量
data1.drop(['gender', 'house', 'segment'], axis=1, inplace=True)

# 用中位数填充age和store_exp
data1['age'].fillna(data1['age'].median(), inplace=True)
data1['store_exp'].fillna(data1['store_exp'].median(), inplace=True)

# 取出income为空的数据作为测试集
test_income = data1.loc[data1['income'].isnull()]
data1.dropna(inplace=True)  # 去除测试集
y_income = data1['income']  # 在预测出点后用于计算平均距离
data1.drop('income', axis=1, inplace=True)
```


```python
# 数据标准化
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(data1)
data2 = ss.transform(data1)
```


```python
data2 = pd.DataFrame(data2, columns=['age', 'store_exp', 'online_exp', 'store_trans', 'online_trans', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10'])
data2.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>store_exp</th>
      <th>online_exp</th>
      <th>store_trans</th>
      <th>online_trans</th>
      <th>Q1</th>
      <th>Q2</th>
      <th>Q3</th>
      <th>Q4</th>
      <th>Q5</th>
      <th>Q6</th>
      <th>Q7</th>
      <th>Q8</th>
      <th>Q9</th>
      <th>Q10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.439428</td>
      <td>-0.290332</td>
      <td>-1.118701</td>
      <td>-0.863047</td>
      <td>-1.571195</td>
      <td>0.686791</td>
      <td>0.154184</td>
      <td>-0.713016</td>
      <td>-0.628257</td>
      <td>-1.677989</td>
      <td>1.165957</td>
      <td>-1.861527</td>
      <td>1.497811</td>
      <td>-1.103135</td>
      <td>1.626789</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.875883</td>
      <td>-0.307717</td>
      <td>-1.232434</td>
      <td>-0.329325</td>
      <td>-1.571195</td>
      <td>0.686791</td>
      <td>-0.695912</td>
      <td>-0.713016</td>
      <td>-0.628257</td>
      <td>-1.677989</td>
      <td>1.165957</td>
      <td>-1.861527</td>
      <td>1.497811</td>
      <td>-2.035942</td>
      <td>1.626789</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.584913</td>
      <td>-0.303363</td>
      <td>-1.132926</td>
      <td>0.471258</td>
      <td>-1.571195</td>
      <td>1.388195</td>
      <td>0.154184</td>
      <td>-0.713016</td>
      <td>-0.628257</td>
      <td>-1.677989</td>
      <td>1.165957</td>
      <td>-1.861527</td>
      <td>1.497811</td>
      <td>-2.035942</td>
      <td>1.626789</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.657656</td>
      <td>-0.351986</td>
      <td>-1.213590</td>
      <td>1.271841</td>
      <td>-1.571195</td>
      <td>1.388195</td>
      <td>0.154184</td>
      <td>-0.713016</td>
      <td>0.219111</td>
      <td>-1.677989</td>
      <td>1.165957</td>
      <td>-1.861527</td>
      <td>1.497811</td>
      <td>-1.103135</td>
      <td>1.626789</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.002973</td>
      <td>-0.341167</td>
      <td>-1.230846</td>
      <td>-0.329325</td>
      <td>-1.315314</td>
      <td>0.686791</td>
      <td>-0.695912</td>
      <td>-0.713016</td>
      <td>0.219111</td>
      <td>-1.677989</td>
      <td>1.165957</td>
      <td>-1.861527</td>
      <td>1.497811</td>
      <td>-1.103135</td>
      <td>1.626789</td>
    </tr>
  </tbody>
</table>
</div>



```python
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=5)
neigh.fit(data2)
points = neigh.kneighbors(test_income.drop('income', axis=1), return_distance=False)
```


```python
mean = []
for i in range(len(test_income)):
    mean.append(y_income.iloc[points[i]].mean()) 

test_income['income'] = mean
```



### Bagging 填充

Bagging是一种集成学习方法，可以用剩余变量训练一个Bagging模型，再用这个模型去预测缺失值，但是它的计算量要大的多。一般来说，如果中位数或者均值填补就能满足建模的需要，使用Bagging的方式填补，就是可以提高一点精度，但是提升的可能会很小，这在样本量很大的使用，就没有太大意义了。

**常用的插值方法有：拉格朗日插值法、牛顿插值法、Hermite插值、分段插值、样条插值法等。这里重点介绍前两种。**




- **1.拉格朗日插值法**

- 根据数学知识可知，对于平面上已知的n个点（无两点在一条直线上）可以找到一个n-1次多项式
  $$
  y = a_0 + a_1x + a_2x^2+ \dots +a_{n-1}x^{n-1}
  $$
  ，使此多项式曲线过这n个点。

  求已知的过n个点的n-1次多项式：
  $$
  y = a_0 + a_1x + a_2x^2+ \dots +a_{n-1}x^{n-1}
  $$
  将n个点的坐标
  $$
  (x_1, y_1),(x_2,y_2) \dots (x_n, y_n)
  $$
  带入多项式函数，得
  $$
  y_1 = a_0 + a_1x + a_2x^2+ \dots +a_{n-1}x^{n-1} \\ 
  y_2 = a_0 + a_1x + a_2x^2+ \dots +a_{n-1}x^{n-1} \\
  \cdots \cdots \\
  y_n = a_0 + a_1x + a_2x^2+ \dots +a_{n-1}x^{n-1}
  $$
  解出拉格朗日插值多项式为：
  $$
  L(x)=\sum^n_{i = 0}y_i \prod^n_{j = 0,j\not= i}\frac{x - x_j}{x_i - x_j}
  $$
  **# 将缺失的函数值对应的点x代入插值多项式得到缺失值的近似值$L(x)$**

- **2.牛顿插值法**

- 拉格朗日插值多项式的优点是格式整齐规范，但其缺点是：当需要增加节点时，其基函数都要发生变化，需要重新计算，这在实际计算中会影响效率。而牛顿插值法可以弥补这一不足。[这里查看更多关于差商公式的介绍](https://wenku.baidu.com/view/57af0aa0b0717fd5360cdc57.html)

  

  拉格朗日插值多项式的优点是格式整齐规范，但其缺点是：当需要增加节点时，其基函数都要发生变化，需要重新计算，这在实际计算中会影响效率。而牛顿插值法可以弥补这一不足。

  **# 求已知的n个点对$(x_1,y_1),(x_2, y_2) \dots (x_n, y_n)$的所有阶差商公式**
  $$
  f[x_1, x] = \frac{f[x]-f[x_1]}{x - x_1} = \frac{f(x) - f(x_1)}{x - x_1} \\
  f[x_2,x_1,x] = \frac{f[x_1,x] - f[x_2,x_1]}{x - x_2} \\
  f[x_3,x_2,x_1,x] = \frac {f[x_2,x_1,x] - f[x_3,x_2,x_1]}{x - x_3} \\
  \cdots\cdots \\
  f[x_n, x_{n-1}, \dots, x_1, x] = \frac{f[x_{n-1}, \dots, x_1, x] - f[x_n, x_{n-1}, \dots, x_1]}{x - x_n}
  $$

**# 联立以上差商公式建立如下插值多项式$f(x)$**
$$
f(x) = f(x_1) + (x-x_1)f[x_2,x_1] + (x-x_1)(x-x_2)f[x_3,x_2,x_1] + \\
(x-x_1)(x-x_2)(x-x_3)f[x_4,x_3,x_2,x_1] + \dots + \\
(x-x_1)(x-x_2)\dots (x-x_{n-1})f[x_n, x_{n-1}, \dots, x_2, x_1] + \\
(x-x_1)(x-x_2)\dots (x-x_n)f[x_n, x_{n-1}, \dots ,x_1, x] \\
= P(x) + R(x)
$$
其中
$$
\begin{align}P(x) = f(x_1) + (x-x_1)f[x_2,x_1] + (x-x_1)(x-x_2)f[x_3,x_2,x_1] + \\
(x-x_1)(x-x_2)(x-x_3)f[x_4,x_3,x_2,x_1] + \dots + \\
(x-x_1)(x-x_2)\dots (x-x_{n-1})f[x_n, x_{n-1}, \dots, x_2, x_1] \\
R(x) = (x-x_1)(x-x_2)\dots(x-x_n)f[x_n, x_{n-1}, \dots,x_1, x] \end{align}
$$
P(x)是牛顿插值逼近函数，R(x)是误差函数。

**# 将缺失的函数对应的点x代入插值多项式得到缺失值的近似值f(x)。**


```python
#拉格朗日插值代码
import pandas as pd #导入数据分析库Pandas
from scipy.interpolate import lagrange #导入拉格朗日插值函数

inputfile = 'data/catering_sale.xls' #销量数据路径
outputfile = 'tmp/sales.xls' #输出数据路径

data = pd.read_excel(inputfile) #读入数据
data[u'销量'][(data[u'销量'] < 400) | (data[u'销量'] > 5000)] = None #过滤异常值，将其变为空值

#自定义列向量插值函数
#s为列向量，n为被插值的位置，k为取前后的数据个数，默认为5
def ployinterp_column(s, n, k=5):
  y = s[list(range(n-k, n)) + list(range(n+1, n+1+k))] #取数
  y = y[y.notnull()] #剔除空值
  return lagrange(y.index, list(y))(n) #插值并返回插值结果

#逐个元素判断是否需要插值
for i in data.columns:
  for j in range(len(data)):
    if (data[i].isnull())[j]: #如果为空即插值。
      data[i][j] = ployinterp_column(data[i], j)

data.to_excel(outputfile) #输出结果，写入文件
```

    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
      if __name__ == '__main__':
    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/pandas/core/series.py:696: FutureWarning: 
    Passing list-likes to .loc or [] with any missing label will raise
    KeyError in the future, you can use .reindex() as an alternative.
    
    See the documentation here:
    http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike
      return self.loc[key]
    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy

在进行插值之前，一般先对异常值进行处理。如将异常值定义为空值，进行插值。

## 异常值处理

异常值如何处理，需视情况而定。

异常值处理常用方法

|    异常值处理方法    |                     方法描述                     |
| :------------------: | :----------------------------------------------: |
| 删除含有异常值的记录 |            直接将含有异常值的记录删除            |
|      视为缺失值      | 将异常值视为缺失值，利用缺失值处理的方法进行处理 |
|      平均值修复      |      可用前后两个观测值的平均值修正该异常值      |
|        不处理        |      直接在具有异常值的数据集上进行挖掘建模      |

在很多情况下，要先分析异常值出现的可能原因，再判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。

### 处理离群点

- 1.箱线图和直方图等一些基本可视化可以用来初步检查是否有离群点。
- 2.除了可视化这样直观的方式外，在一定的假设条件下，还有一些统计学的定义离群值的方法。如常用Z分值来判断可能的离群点。



对于某观测变量Y的Z分值定义为：
$$
Z_i = \frac{Y_i - \bar Y}{s}
$$

离群点的影响取决于你使用的模型，有的模型对离群值很敏感，如线性回归、逻辑回归。有的模型对离群点具有抗性，如基于树的模型、支持向量机模型。此外，**离群点和错误的观测不一样，它是真实的观测，其中包含信息，所以不能随意删除。**

如果你使用的模型对离群点非常敏感，可以使用空间表示变换。该变换将自变量取值映射到高纬的球面上。变换公式如下：
$$
xX^*_{ij} = \frac{x_{ij}}{\sqrt{\sum_{j=1}^{p}{x^2_{ij}}}}
$$

其中$x_{ij}$表示第i个样本对应第j个变量的观测。由公式可见，每个样本都除以它们的平方模。公式的分母其实可以看作是该样本到p维空间0点的欧氏距离，有以下三点需要特别注意：

其中$x_{ij}$表示第i个样本对应第j个变量的观测。由公式可见，每个样本都除以它们的平方模。公式的分母其实可以看作是该样本到p维空间0点的欧氏距离，有以下三点需要特别注意：

- 1.在变换前需要对自变量标准化
- 2.于中心化和标准化不用，这个变换操作的对象是所有的自变量。
- 3.如果需要移除变量，这一步必须要在空间表示变换之前，否则会导致一系列问题。

## 数据集成

数据集成是将多个数据源合并存放在一个一致的数据存储中的过程。

### 实体识别

实体识别的任务是统一不同源数据的矛盾。

- **1.同名异义**
  数据源A中的属性ID和数据源B中的属性ID分别描述的是菜品编号和订单编号，即描述的是不同的实体。
- **2.异名同义**
  数据源A中的sales_dt 和数据源B中的sales_date 都是描述销售日期的。
- **3.单位不统一**br<> 描述同一个实体分别用的是国际单位和中国传统的计量单位。

### 冗余属性识别

数据集成往往导致数据冗余，例如：

- 1.同一属性多次出现
- 2.同一属性命名不一致导致重复。

对于冗余属性要先分析，检测到后再将其删除。

有些冗余属性可以用相关性分析检测。给定两个数值型的属性A和B，根据其属性值 ，用相关系数度量一个属性在多大程度上蕴含另一个属性。

## 数据变换

数据变换主要是对数据进行规范化处理，将数据转换成“适当的”形式，以适用于挖掘任务及算法的需要。

### 有偏分布

如果模型要求变量服从一定的对称分布（如正态分布），则需要进行数据变换去除分布中的偏度。

> 偏度是3阶标准化中心[矩](https://zh.wikipedia.org/wiki/%E7%9F%A9_(%E6%95%B8%E5%AD%B8)),是用来衡量分布不对称程度的，该统计两的数学定义如下：
> $$
> 偏度 = \frac{\sum(X_i + \bar x)^3}{(n - 1)v^\frac {2}{3}}
> $$
>
> $$
> v=\frac{\sum (x_i = \bar x)^2}{(n-1)}
> $$
>
>
> 数据分布对称时偏度=0，分布左偏时偏度<0，分布右偏时偏度>0，且偏离程度越大，偏度统计量的绝对值越大。

有很多变换有助于去除偏度，如log变换、平方根或者取倒数。Box和Cox（1964）提出了含有一个参数\lambda的指数变换族：
$$
x^* = \begin{cases}
\frac{x^\lambda - 1}{\lambda}, if(\lambda \ne 0) \\ \log (x), if(\lambda = 0)
\end{cases}
$$
很容易看出这个变换族群包含了log(x)变换（$\lambda = 0$）、$x^2$变换（$\lambda = 2$）、sqrt(x)变换（$\lambda = 0.5$），以及$fraclx$变换（$\lambda = -1$）等常用的变换。Box-Cox覆盖的面更广，变换指数可能是任意实数。

### 简单函数变换

简单的函数变换常用来将不具有正态分布的数据变换成具有正态分布的数据。在时间序列分析中，有时简单的对数变换或者差分运算就可以将非平稳序列转换成平稳序列。常用简单函数变换：
$$
x'=x^2 \\ x'=\sqrt x \\ x'=log(x) \\ \nabla f(x_k) = f(x_{k+1}) - f(x_k)
$$


### 中心化和标准化

这是最基本的数据变换。

- 1.中心化是通过将变量的每个观测减去该变量均值，这样中心化后的变量观测值为0。
- 2.标准化是将变量观测除以变量标准差，标准化后的变量标准差为1。

对于一些要对变量进行线性组合的模型，中心化和标准化保证了变量的线性组合是基于组合后的新变量能够解释的 原始变量中的方差。用到基于方差的变量线性组合的模型有主成分分析(PCA)、偏最小二乘分析(PLS)、探索因子分析(EFA)等。

通过参数估计衡量各个自变量和因变量之间关系强度时，必须要对变量观测进行标准化。在仅需要确保参数被“公平”对待时，有时只需要标准化数据而不一定要中心化。这是对数据收缩常用方法。

$$
X^*_{i,j} = \frac{X_{i,j} - quantile(X_{i,j}, 0.01)}{quantile(X_{i,j}, 0.99) - quantile(X_{i,j}, 0.01)}
$$
这里的$X_{i,j}$代表第个样本的第j个变量观测，$quantile(X_{i,j}, 0.01)$指的是第j个变量所有样本观测组成的向量1%分位数，类似地，$quantile(X_{i,j}, 0.99)$是99%分位数，这里之所以使用99%和1%而非最大最小值，是为了减弱离群点的影响。

### 规范化（归一化）

为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，使之落入一个特定的区域，便于进行综合分析。

**数据规范化对于基于距离的挖掘算法尤为重要**

**常用规范化方法**

- **1.最小-最大规范化**

  最小-最大规范化也称为离差标准化，是对原始数据的线性变换，将数据值映射到[0, 1]之间。
  $$
  x^*=\frac{x - min}{max-min}
  $$
  这是最简单的方法，缺点是：若数据集中且某个数值很大，则规范化后各值会接近于0，并且将会相差不打；若将来有新的数据加入，并且不在区间[min, max]内，则需要重新选取最大和最小值，重新进行计算。

- **2.零-均值规范化**

  零-均值规范化也称为标准差标准化，经过处理的数据的均值为0，标准差为1。公式为：
  $$
  x^*=\frac{x - \bar x}{\sigma}
  $$

  其中$\bar x$为原始数据的均值，$\sigma$为原始数据的标准差，是当前用得最多的数据标准化方法。

- **3.小数标定规范化**

  通过移动属性值的小数位数，将属性值映射到[-1, 1]之间，移动的小数位数取决于属性值绝对值的最大值。公式为：
  $$
  x^*=\frac{x}{10^k}
  $$


```python
#数据规范化
import pandas as pd
import numpy as np

datafile = 'data/normalization_data.xls' #参数初始化
data = pd.read_excel(datafile, header = None) #读取数据

(data - data.min())/(data.max() - data.min()) #最小-最大规范化
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.074380</td>
      <td>0.937291</td>
      <td>0.923520</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.619835</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.850941</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.214876</td>
      <td>0.119565</td>
      <td>0.813322</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.563676</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.000000</td>
      <td>0.942308</td>
      <td>0.996711</td>
      <td>0.804149</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.264463</td>
      <td>0.838629</td>
      <td>0.814967</td>
      <td>0.909310</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.636364</td>
      <td>0.846990</td>
      <td>0.786184</td>
      <td>0.929571</td>
    </tr>
  </tbody>
</table>


```python
(data - data.mean())/data.std() #零-均值规范化
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.905383</td>
      <td>0.635863</td>
      <td>0.464531</td>
      <td>0.798149</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.604678</td>
      <td>-1.587675</td>
      <td>-2.193167</td>
      <td>0.369390</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.516428</td>
      <td>-1.304030</td>
      <td>0.147406</td>
      <td>-2.078279</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.111301</td>
      <td>0.784628</td>
      <td>0.684625</td>
      <td>-0.456906</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.657146</td>
      <td>0.647765</td>
      <td>0.675159</td>
      <td>0.234796</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.379150</td>
      <td>0.401807</td>
      <td>0.152139</td>
      <td>0.537286</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.650438</td>
      <td>0.421642</td>
      <td>0.069308</td>
      <td>0.595564</td>
    </tr>
  </tbody>
</table>


```python
data/10**np.ceil(np.log10(data.abs().max())) #小数定标规范化
```





<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.078</td>
      <td>0.521</td>
      <td>0.602</td>
      <td>0.2863</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.144</td>
      <td>-0.600</td>
      <td>-0.521</td>
      <td>0.2245</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.095</td>
      <td>-0.457</td>
      <td>0.468</td>
      <td>-0.1283</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.069</td>
      <td>0.596</td>
      <td>0.695</td>
      <td>0.1054</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.190</td>
      <td>0.527</td>
      <td>0.691</td>
      <td>0.2051</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.101</td>
      <td>0.403</td>
      <td>0.470</td>
      <td>0.2487</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.146</td>
      <td>0.413</td>
      <td>0.435</td>
      <td>0.2571</td>
    </tr>
  </tbody>
</table>



### 连续属性离散化

一些数据挖掘算法，特别是某些分类算法（如ID3算法，Apriori 算法等），要求数据是分类属性形式。这样，常常需要将连续属性变换成分类属性，即连续属性离散化。

**1.离散化的过程**

连续属性离散化是将取值范围划分为一些离散话的区间，最后用不同的符号或整数值，代表落在每个子区间中的数据值。所以离散化涉及两个子任务：**确定分类数以及如何将连续属性值映射到这些分类值。**

**2.常用的离散化方法**

- 等宽法

  将属性的值域分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者由用户指定。**缺点**在于它对离群点比较敏感，倾向于不均匀地把属性值分布到各个区间。而等频法正能避免这个问题。

- 等频法

  将相同数量的记录放进每个区间。**缺点**是可能将相同数据值分到不同的区间以满足每个区间中固定的数据个数。

- 基于聚类分析的方法（一维）

  首先将连续属性的值用聚类算法（如K-Mean 算法）进行聚类，然后再将聚类得到的簇进行处理，合并到一个簇的连续属性值并做同一标记。聚类算法需要用户指定簇的个数，从而决定产生的区间。



> 个人不建议分析师自行地将连续变量离散化，除非客户或相关领域专家给出明确的理由。连续变量的效能通常比区间变量高，你需要权衡将连续变量离散化对可解释性的提升和对模型精确度的损害。注意这里指的是人为主观地将一些连续变量转变为分类变量而非模型检测出的截断点。有一些模型，如分类/回归树和多元自适应回归样条，它们在建模过程中能够估计合适的截断点。但这属于建模，而非数据预处理。


```python
datafile = 'data/discretization_data.xls' #参数初始化
data = pd.read_excel(datafile) #读取数据
data = data[u'肝气郁结证型系数'].copy()
k = 4

d1 = pd.cut(data, k, labels = range(k)) #等宽离散化，各个类比依次命名为0,1,2,3

#等频率离散化
w = [1.0*i/k for i in range(k+1)]
w = data.describe(percentiles = w)[4:4+k+1] #使用describe函数自动计算分位数
w[0] = w[0]*(1-1e-10)
d2 = pd.cut(data, w, labels = range(k))

from sklearn.cluster import KMeans #引入KMeans
kmodel = KMeans(n_clusters = k, n_jobs = 4) #建立模型，n_jobs是并行数，一般等于CPU数较好
kmodel.fit(data.reshape((len(data), 1))) #训练模型
c = pd.DataFrame(kmodel.cluster_centers_).sort_values(0) #输出聚类中心，并且排序（默认是随机序的）
w = pd.rolling_mean(c, 2).iloc[1:] #相邻两项求中点，作为边界点
w = [0] + list(w[0]) + [data.max()] #把首末边界点加上
d3 = pd.cut(data, w, labels = range(k))

def cluster_plot(d, k): #自定义作图函数来显示聚类结果
  import matplotlib.pyplot as plt
  
  plt.figure(figsize = (8, 3))
  for j in range(0, k):
    plt.plot(data[d==j], [j for i in d[d==j]], 'o')
  
  plt.ylim(-0.5, k-0.5)
  return plt

cluster_plot(d1, k).show()
cluster_plot(d2, k).show()
cluster_plot(d3, k).show()
```

    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:16: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead
      app.launch_new_instance()
    /home/heolis/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:18: FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with 
    	DataFrame.rolling(window=2,center=False).mean()



![png](D:/google下载/数据预处理介绍/output_37_1.png)



![png](D:/google下载/数据预处理介绍/output_37_2.png)



![png](D:/google下载/数据预处理介绍/output_37_3.png)


### 属性构造

利用已有的属性集构造出新的属性，并加入到现有的属性集合中。这属于特征工程的内容，以后会单独介绍。

### 小波变换

小波变换是一种新型的数据分析工具，它能够刻画某个问题的特征量往往是隐含在一个信号中的某个或某些分量中，小波变换可以把非平稳信号分解为表达不同层次、不同频率信息的数据序列，即小波系数。选取适当的小波系数，即完成了信号的特征提取。小波变换内容比较多，这里暂时只做分类，以后再详细介绍。

- **1.基于小波变换的特征提取方法**
 - 基于小波变换的多尺度空间能量分布特征提取方法
 - 基于小波变换的多尺度空间的模极大值特征提取方法
 - 基于小波包变换的特征提取方法
 - 基于自适应小波神经网络的特征提取方法
- **2.小波基函数**
- **3.小波变换**


在Python中，有一个较为完善的信号处理库可以利用。`PyWavelets`

## 数据规约

数据规约的意义在于：

- 降低无效、错误数据对建模的影响，提高建模的准确性
- 少量且具有代表性的数据将大幅缩减数据挖掘所需的时间
- 降低存储数据的成本

### 属性规约

属性规约的目标是寻找 出最小的属性子集并确保新数据子集的概率分布尽可能地接近原来数据集的概率分布。属性规约常用的有一下几种方法：

- 合并属性：将旧属性合并为新属性
- 逐步向前选择
- 逐步向后删除
- 决策树归纳：用决策树对原始数据进行建模，没有出现在这个决策树上的属性均可认为是无关属性。
- 主成分分析


```python
#主成分分析 降维
import pandas as pd

#参数初始化
inputfile = 'data/principal_component.xls'
outputfile = 'tmp/dimention_reducted.xls' #降维后的数据

data = pd.read_excel(inputfile, header = None) #读入数据

from sklearn.decomposition import PCA

pca = PCA()
pca.fit(data)
pca.components_ #返回模型的各个特征向量
```




    array([[ 0.56788461,  0.2280431 ,  0.23281436,  0.22427336,  0.3358618 ,
             0.43679539,  0.03861081,  0.46466998],
           [ 0.64801531,  0.24732373, -0.17085432, -0.2089819 , -0.36050922,
            -0.55908747,  0.00186891,  0.05910423],
           [-0.45139763,  0.23802089, -0.17685792, -0.11843804, -0.05173347,
            -0.20091919, -0.00124421,  0.80699041],
           [-0.19404741,  0.9021939 , -0.00730164, -0.01424541,  0.03106289,
             0.12563004,  0.11152105, -0.3448924 ],
           [-0.06133747, -0.03383817,  0.12652433,  0.64325682, -0.3896425 ,
            -0.10681901,  0.63233277,  0.04720838],
           [ 0.02579655, -0.06678747,  0.12816343, -0.57023937, -0.52642373,
             0.52280144,  0.31167833,  0.0754221 ],
           [-0.03800378,  0.09520111,  0.15593386,  0.34300352, -0.56640021,
             0.18985251, -0.69902952,  0.04505823],
           [-0.10147399,  0.03937889,  0.91023327, -0.18760016,  0.06193777,
            -0.34598258, -0.02090066,  0.02137393]])




```python
pca.explained_variance_ratio_ #返回各个成分各自的方差百分比
```




    array([  7.74011263e-01,   1.56949443e-01,   4.27594216e-02,
             2.40659228e-02,   1.50278048e-03,   4.10990447e-04,
             2.07718405e-04,   9.24594471e-05])



可以看出前4个主成分，累积贡献率已达到97.37%，说明选取前3个主成分进行计算已经相当不错了，因此可以重新建立PCA模型，设置n_components=3，计算出成分结果。

### 数据规约

数值规约指通过选择替代的、较小的数据来减少数据量。常用方法有

- 直方图法
- 聚类
- 抽样
 - 聚类抽样
 - 分层抽样
- 参数回归


### 共线性 

即相关性，我们可以绘制相关性矩阵图，可视化变量之间的共线性。

两个变量相关性是不是越强越好呢，不然。两个变量高度相关意味着它们含有重复的信息，我们其实不需要将两个变量同时留在模型中。变量高度相关会导致参数估计极为不稳定，所以我们在进行回归之前需要移除一些高度相关的变量，使得模型中变量相关性在一定的范围之内。《应用预测模型》一书中在处理 该问题时的核心思想是**在删除尽可能少的变量的情况下，将变量两两相关性控制在人为设定的一个阈值内。**

> **处理高度相关变量的算法如下：**

- 1.计算自变量的相关系数矩阵；
- 2.找出相关系数绝对值最大的那对自变量（记为自变量A和B）;
- 3.计算 A 和其他自变量相关系数的均值，对 B 也做同样的计算;
- 4.如果 A 的平均相关系数更大，则将 A 移除；如若不然，移除 B;
- 5.重复步骤2到4，直至所有相关系数的绝对值都低于设定的阈值为止。

建议将这个阈值当成一个调优参数，试验不用的值，看那个对应的模型精度更高，建议在0.6~0.8范围内寻找最优的阈值。

## 稀疏变量

除了高度相关的变量以外，我们还需要移除那些观测非常稀疏的变量。一个极端的例子是某变量观测只有一个取值，我们可以将其称为0方差变量。有的可能 只有若干取值，我们称其为近0方差变量。我们需要识别这些变量，然后将其删除。这些变量的存在对如线性回归和逻辑回归这杨2的模型拟合的表现和稳定性会有很大影响，但对决策树模型没有影响。<br>
**通常识别这样的变量有两个法则**

- 不同取值数目和样本量的比值；
- 最常见的取值频数和第二常见的取值频数之间的比值。

具体怎样处理这些变量，需要去试验一下，哪个方法得到的模型精度高就用哪个。

## 编码名义变量

名义变量，又称为虚设变量，是一个指标性质的变量，通常取值0或1。有时你需要将分类变量转化成名义变量，例如在一份问卷中，每个问题有A,B,C,D,E 五个选项，通常应该将其转化为五个名义变量，然后将其中一个选项当作基准选项。



# -------------------------------------1

# -------------------------------------2

# 数据预处理-机器学习初步

https://github.com/MLEveryday/100-Days-Of-ML-Code

## 数据预处理实现

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115140703.png" alt="image-20200728115140703" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115217380.png" alt="image-20200728115217380" style="zoom:50%;" />

#### 代码

###### 第1步：导入库

```
import numpy as np
import pandas as pd
```

###### 第2步：导入数据集

```
dataset = pd.read_csv('Data.csv')//读取csv文件
X = dataset.iloc[ : , :-1].values//.iloc[行，列]
Y = dataset.iloc[ : , 3].values  // : 全部行 or 列；[a]第a行 or 列
                                 // [a,b,c]第 a,b,c 行 or 列
```

###### 第3步：处理丢失数据

```
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
```

###### 第4步：解析分类数据

```
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
```

###### 创建虚拟变量

```
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
```

###### 第5步：拆分数据集为训练集合和测试集合

```
#from sklearn.model_selection import train_test_split
from sklearn.cross_validation import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
```

###### 第6步：特征量化

```
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
```

#### 面试问题



## 简单线性回归

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115543710.png" alt="image-20200728115543710" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115615929.png" alt="image-20200728115615929" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115631477.png" alt="image-20200728115631477" style="zoom:50%;" />

第一步：数据预处理

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv('studentscores.csv')
X = dataset.iloc[ : ,   : 1 ].values
Y = dataset.iloc[ : , 1 ].values

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 1/4, random_state = 0) 
```

第二步：训练集使用简单线性回归模型来训练

```
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor = regressor.fit(X_train, Y_train)
```

第三步：预测结果

```
Y_pred = regressor.predict(X_test)
```

第四步：可视化

训练集结果可视化

```
plt.scatter(X_train , Y_train, color = 'red')
plt.plot(X_train , regressor.predict(X_train), color ='blue')
plt.show()
```

测试集结果可视化

```
plt.scatter(X_test , Y_test, color = 'red')
plt.plot(X_test , regressor.predict(X_test), color ='blue')
plt.show()
```

## 多远线性回归

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728115959654.png" alt="image-20200728115959654" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120034425.png" alt="image-20200728120034425" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120202085.png" alt="image-20200728120202085" style="zoom:50%;" />

第1步: 数据预处理

导入库

```
import pandas as pd
import numpy as np
```

导入数据集

```
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[ : , :-1].values
Y = dataset.iloc[ : ,  4 ].values
```

将类别数据数字化

```
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
X[: , 3] = labelencoder.fit_transform(X[ : , 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray()
```

躲避虚拟变量陷阱

```
X = X[: , 1:]
```

拆分数据集为训练集和测试集

```
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)
```

第2步： 在训练集上训练多元线性回归模型

```
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, Y_train)
```

Step 3: 在测试集上预测结果

```
y_pred = regressor.predict(X_test)
```

## 逻辑回归

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120332715.png" alt="image-20200728120332715" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120409746.png" alt="image-20200728120409746" style="zoom:50%;" />

数据集 | 社交网络

[![img](https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Other%20Docs/data.png?raw=true)](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other Docs/data.png?raw=true)

该数据集包含了社交网络中用户的信息。这些信息涉及用户ID,性别,年龄以及预估薪资。一家汽车公司刚刚推出了他们新型的豪华SUV，我们尝试预测哪些用户会购买这种全新SUV。并且在最后一列用来表示用户是否购买。我们将建立一种模型来预测用户是否购买这种SUV，该模型基于两个变量，分别是年龄和预计薪资。因此我们的特征矩阵将是这两列。我们尝试寻找用户年龄与预估薪资之间的某种相关性，以及他是否购买SUV的决定。

步骤1 | 数据预处理

导入库

```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

导入数据集

[这里](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/Social_Network_Ads.csv)获取数据集

```
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
Y = dataset.iloc[:,4].values
```

将数据集分成训练集和测试集

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)
```

特征缩放

```
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

步骤2 | 逻辑回归模型

该项工作的库将会是一个线性模型库，之所以被称为线性是因为逻辑回归是一个线性分类器，这意味着我们在二维空间中，我们两类用户（购买和不购买）将被一条直线分割。然后导入逻辑回归类。下一步我们将创建该类的对象，它将作为我们训练集的分类器。

将逻辑回归应用于训练集

```
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
```

步骤3 | 预测

预测测试集结果

```
y_pred = classifier.predict(X_test)
```

步骤4 | 评估预测

我们预测了测试集。 现在我们将评估逻辑回归模型是否正确的学习和理解。因此这个混淆矩阵将包含我们模型的正确和错误的预测。

生成混淆矩阵

```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```

可视化

```
from matplotlib.colors import ListedColormap
X_set,y_set=X_train,y_train
X1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),
                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(),X1.max())
plt.ylim(X2.min(),X2.max())
for i,j in enumerate(np. unique(y_set)):
    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],
                c = ListedColormap(('red', 'green'))(i), label=j)

plt. title(' LOGISTIC(Training set)')
plt. xlabel(' Age')
plt. ylabel(' Estimated Salary')
plt. legend()
plt. show()

X_set,y_set=X_test,y_test
X1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),
                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(),X1.max())
plt.ylim(X2.min(),X2.max())
for i,j in enumerate(np. unique(y_set)):
    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],
                c = ListedColormap(('red', 'green'))(i), label=j)

plt. title(' LOGISTIC(Test set)')
plt. xlabel(' Age')
plt. ylabel(' Estimated Salary')
plt. legend()
plt. show()
```

[![img](https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Other%20Docs/LR_training.png?raw=true)](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other Docs/LR_training.png?raw=true) [![img](https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Other%20Docs/LR_test.png?raw=true)](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other Docs/LR_test.png?raw=true)

##### 参考文献

https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc

## K近邻法

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120755163.png" alt="image-20200728120755163" style="zoom:67%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728120838057.png" alt="image-20200728120838057" style="zoom:67%;" />

使用K-NN对训练集数据进行训练

```
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)
```

对测试集进行预测

```
y_pred = classifier.predict(X_test)
```

生成混淆矩阵

```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```

## SVM支持向量机

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121148263.png" alt="image-20200728121148263" style="zoom:67%;" />



<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121253513.png" alt="image-20200728121253513" style="zoom:67%;" />![image-20200728121331076](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121331076.png)

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121339852.png" alt="image-20200728121339852" style="zoom:67%;" />

特征量化

```
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)
```

适配SVM到训练集合

```
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)
```

预测测试集合结果

```
y_pred = classifier.predict(X_test)
```

创建混淆矩阵

```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```

训练集合结果可视化

```
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```

[![img](https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Other%20Docs/SVM_training%20set.png)](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other Docs/SVM_training set.png)

测试集合结果可视化

```
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```

[![img](https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Other%20Docs/SVM_test%20set.png)](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other Docs/SVM_test set.png)

## 决策树

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121613851.png" alt="image-20200728121613851" style="zoom:67%;" />![image-20200728121700425](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121700425.png)



<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728121709501.png" alt="image-20200728121709501" style="zoom:67%;" />

特征缩放

```
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

对测试集进行决策树分类拟合

```
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)
```

预测测试集的结果

```
y_pred = classifier.predict(X_test)
```

制作混淆矩阵

```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```

将训练集结果进行可视化

```
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```

将测试集结果进行可视化

```
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```

## K-均值聚类

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122116911.png" alt="image-20200728122116911" style="zoom:67%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122131871.png" alt="image-20200728122131871" style="zoom:67%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122200131.png" alt="image-20200728122200131" style="zoom:67%;" />

## 层次聚类

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122630536.png" alt="image-20200728122630536" style="zoom:67%;" /><img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122700069.png" alt="image-20200728122700069" style="zoom:67%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122706990.png" alt="image-20200728122706990" style="zoom:67%;" /><img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200728122723540.png" alt="image-20200728122723540" style="zoom:67%;" />

# -------------------------------------2

# DW数据挖掘

#### 第一章

```
os.getcwd() 查询当前工作目录，当当前工作路径错误时
```

##### **逐块读取**

```
chunker = pd.read_csv('', chunksize = 1000)
```

![image-20200819161315552](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161315552.png)

##### **确认表头名属性名**

```
pd.read_csv( names = [], index_col = '')
```

![image-20200819161406147](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161406147.png)

**数据信息**

```
data.info()
```

**数据为空**

```
data.isnull().head()
```

**删除列**

```
del data['xxx']
```

![image-20200819161523946](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161523946.png)

**元素隐藏**

```
data.drop(['xxx','xxx','xxx'],axis=1).head(5)

inplace = True
```

![image-20200819161649652](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161649652.png)

**筛选条件**

```
df[df['Age']<10]

df[(df['Age']>10)&(df['Age']<50)]
```

![image-20200819161810153](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161810153.png)

![image-20200819161823120](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161823120.png)

**重置索引**

```
df.reset_index()
df.reset_index(inplace = True)
```

![image-20200819161858098](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161858098.png)

**loc和iloc**

![image-20200819161912604](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161912604.png)

![image-20200819161920441](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161920441.png)

![image-20200819161929728](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819161929728.png)

**reshape**

```
reshape重塑  reshape(2,4)
```

##### **sord_Index()**

![image-20200819162017354](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819162017354.png)

![image-20200819162023642](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819162023642.png)

![image-20200819162034435](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819162034435.png)

![image-20200819162040888](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819162040888.png)

##### **sort_values**

```
sort_values 这个可以根据属性值进行排列
```

![image-20200819162126245](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819162126245.png)

#### 第二章

**缺失值**

```
df.info()

df.isnull().sum()
```

**删除重复值**

```
df.drop_duplicates().head()
```

**特征处理**

```
数值型特征
文本型特征
```

![image-20200819192917188](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819192917188.png)

![image-20200819192923942](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819192923942.png)

**数值转换**

```
df['new'] = pd.cut(df['xxx'],5,labels=['1','2','3','4','5'])
```

![image-20200819193015948](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193015948.png)

```
df['new'] = pd.cut(df['xxx'],[0,5,15,30,50,80],labels = ['1','2','3','4','5'])
```

![image-20200819193102052](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193102052.png)

```
df['new'] = pd.qcut(df['xxx'],[0,0.1,0.3,0.5,0.7,0.9],labels = ['1','2','3','4','5'])
```

**值统计**

```
df['xxx'].value_counts()
```

**唯一值**

```
df['xxx'].unique()
df['xxx'].nunique()
```

**文本转换**

**replace**

![image-20200819193512656](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193512656.png)

```
df['xxx'] = df['Sex'].replace(['male','female'],[1,2])
```

**map**

![image-20200819193555488](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193555488.png)

```
df['xxx'] = df['Sex'].map({'male':1,'female':2})
```

**Labelencoder**

![image-20200819193634721](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193634721.png)

**OnehotEncoder**

![image-20200819193715920](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193715920.png)

**提取特征值**

![image-20200819193738452](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200819193738452.png)

